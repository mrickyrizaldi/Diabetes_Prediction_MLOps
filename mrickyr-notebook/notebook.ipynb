{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a7b19c3",
   "metadata": {},
   "source": [
    "# Pipeline Orchestration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48018eb",
   "metadata": {},
   "source": [
    "Nama    : Muhammad Ricky Rizaldi  \n",
    "Email   : mrickyrizaldi@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84e6c91",
   "metadata": {},
   "source": [
    "## Informasi Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecbeae92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"../data/diabetes.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f40cc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Pregnancies               768 non-null    int64  \n",
      " 1   Glucose                   768 non-null    int64  \n",
      " 2   BloodPressure             768 non-null    int64  \n",
      " 3   SkinThickness             768 non-null    int64  \n",
      " 4   Insulin                   768 non-null    int64  \n",
      " 5   BMI                       768 non-null    float64\n",
      " 6   DiabetesPedigreeFunction  768 non-null    float64\n",
      " 7   Age                       768 non-null    int64  \n",
      " 8   Outcome                   768 non-null    int64  \n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8604c723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.845052</td>\n",
       "      <td>120.894531</td>\n",
       "      <td>69.105469</td>\n",
       "      <td>20.536458</td>\n",
       "      <td>79.799479</td>\n",
       "      <td>31.992578</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>33.240885</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.369578</td>\n",
       "      <td>31.972618</td>\n",
       "      <td>19.355807</td>\n",
       "      <td>15.952218</td>\n",
       "      <td>115.244002</td>\n",
       "      <td>7.884160</td>\n",
       "      <td>0.331329</td>\n",
       "      <td>11.760232</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>140.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>127.250000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>0.626250</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   768.000000  768.000000     768.000000     768.000000  768.000000   \n",
       "mean      3.845052  120.894531      69.105469      20.536458   79.799479   \n",
       "std       3.369578   31.972618      19.355807      15.952218  115.244002   \n",
       "min       0.000000    0.000000       0.000000       0.000000    0.000000   \n",
       "25%       1.000000   99.000000      62.000000       0.000000    0.000000   \n",
       "50%       3.000000  117.000000      72.000000      23.000000   30.500000   \n",
       "75%       6.000000  140.250000      80.000000      32.000000  127.250000   \n",
       "max      17.000000  199.000000     122.000000      99.000000  846.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
       "count  768.000000                768.000000  768.000000  768.000000  \n",
       "mean    31.992578                  0.471876   33.240885    0.348958  \n",
       "std      7.884160                  0.331329   11.760232    0.476951  \n",
       "min      0.000000                  0.078000   21.000000    0.000000  \n",
       "25%     27.300000                  0.243750   24.000000    0.000000  \n",
       "50%     32.000000                  0.372500   29.000000    0.000000  \n",
       "75%     36.600000                  0.626250   41.000000    1.000000  \n",
       "max     67.100000                  2.420000   81.000000    1.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1deb6e",
   "metadata": {},
   "source": [
    "Dataset yang digunakan adalah dataset diabetes yang berisi data medis pasien dengan berbagai atribut seperti jumlah kehamilan (Pregnancies), kadar glukosa darah (Glucose), tekanan darah (BloodPressure), ketebalan kulit (SkinThickness), kadar insulin (Insulin), indeks massa tubuh (BMI), fungsi silsilah diabetes (DiabetesPedigreeFunction), dan usia (Age). Kolom Outcome merupakan label target dengan nilai 1 menandakan pasien terdiagnosis diabetes dan 0 menandakan tidak.\n",
    "\n",
    "Sumber dataset: [Kaggle-Diabetes](https://www.kaggle.com/datasets/akshaydattatraykhare/diabetes-dataset/data)\n",
    "\n",
    "Tujuan proyek ini adalah membangun model klasifikasi biner untuk memprediksi apakah seseorang berpotensi mengidap diabetes berdasarkan data medis tersebut. Solusi machine learning yang dikembangkan akan menggunakan pipeline TFX agar setiap tahap mulai dari data ingestion, preprocessing, training, hingga deployment dapat berjalan otomatis dan terstruktur. Target utama proyek ini adalah menghasilkan model yang akurat serta mudah direproduksi untuk evaluasi dan penerapan di lingkungan produksi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a385a1",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "207b2b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "from absl import logging as absl_logging\n",
    "\n",
    "# Komponen utama TFX\n",
    "from tfx.components import (\n",
    "    CsvExampleGen, StatisticsGen, SchemaGen, ExampleValidator,\n",
    "    Transform, Trainer, Tuner, Evaluator, Pusher\n",
    ")\n",
    "\n",
    "# Orchestrator\n",
    "from tfx.orchestration import pipeline as tfx_pipeline\n",
    "from tfx.orchestration import metadata\n",
    "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner\n",
    "\n",
    "# (opsional) jika nanti mau bikin PipelineOptions sendiri\n",
    "# import apache_beam as beam\n",
    "\n",
    "# Resolver untuk latest blessed model\n",
    "from tfx.dsl.components.common.resolver import Resolver\n",
    "from tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy import (\n",
    "    LatestBlessedModelStrategy\n",
    ")\n",
    "from tfx.types import Channel\n",
    "from tfx.types.standard_artifacts import Model, ModelBlessing\n",
    "\n",
    "# Protos\n",
    "from tfx.proto import example_gen_pb2\n",
    "from tfx.proto import trainer_pb2\n",
    "from tfx.proto import pusher_pb2\n",
    "\n",
    "# Evaluasi model\n",
    "import tensorflow_model_analysis as tfma\n",
    "\n",
    "# Pengaturan logging\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "absl_logging.set_verbosity(absl_logging.ERROR)\n",
    "logging.getLogger('apache_beam').setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02572c72",
   "metadata": {},
   "source": [
    "Pada bagian ini, saya memuat seluruh library yang dibutuhkan untuk membangun pipeline TFX secara menyeluruh. Komponen inti seperti CsvExampleGen, StatisticsGen, SchemaGen, Transform, Trainer, hingga Pusher digunakan untuk menjalankan setiap tahapan penting dalam alur machine learning, mulai dari membaca data mentah, menghasilkan statistik dan schema, melakukan preprocessing, melatih model, mengevaluasi performanya, hingga menyiapkan artefak model untuk proses deployment.\n",
    "\n",
    "Saya juga mengimpor modul orkestrasi seperti tfx_pipeline, metadata, dan BeamDagRunner yang berfungsi sebagai eksekutor utama pipeline. Selain itu, Resolver dengan strategi LatestBlessedModelStrategy digunakan untuk memilih model terbaik yang sudah lolos proses evaluasi sebelumnya. Library tambahan seperti tfma membantu melakukan analisis evaluasi model yang lebih komprehensif. Pada akhir bagian, saya menyesuaikan level logging agar output notebook tetap bersih dan lebih mudah untuk dipantau."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce256612",
   "metadata": {},
   "source": [
    "## Set Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4717196e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE_ROOT     : d:\\dicoding\\Submission_MLOps_2\\mrickyr-notebook\\pipelines\\mrickyr-pipeline\n",
      "METADATA_PATH     : d:\\dicoding\\Submission_MLOps_2\\mrickyr-notebook\\metadata\\mrickyr-pipeline\\metadata.db\n",
      "SERVING_MODEL_DIR : d:\\dicoding\\Submission_MLOps_2\\mrickyr-notebook\\serving_model\\mrickyr-pipeline\n",
      "DATA_ROOT         : d:\\dicoding\\Submission_MLOps_2\\data\n",
      "MODULES_DIR       : d:\\dicoding\\Submission_MLOps_2\\mrickyr-notebook\\modules\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = os.getcwd()   # Direktori kerja utama (lokasi notebook)\n",
    "PROJECT_ROOT = os.path.dirname(BASE_DIR) # Direktori proyek (satu tingkat di atas)\n",
    "\n",
    "# Nama komponen pipeline\n",
    "PIPELINE_NAME = \"mrickyr-pipeline\"\n",
    "SCHEMA_PIPELINE_NAME = \"diabetes-tfdv-schema\"\n",
    "\n",
    "# Struktur direktori artefak pipeline\n",
    "PIPELINE_ROOT = os.path.join(BASE_DIR, \"pipelines\", PIPELINE_NAME)\n",
    "METADATA_PATH = os.path.join(BASE_DIR, \"metadata\", PIPELINE_NAME, \"metadata.db\")\n",
    "SERVING_MODEL_DIR = os.path.join(BASE_DIR, \"serving_model\", PIPELINE_NAME)\n",
    "MODULES_DIR = os.path.join(BASE_DIR, \"modules\")\n",
    "\n",
    "# Direktori data\n",
    "DATA_ROOT = os.path.join(PROJECT_ROOT, \"data\")\n",
    "\n",
    "# Inisialisasi direktori yang diperlukan\n",
    "for p in [\n",
    "    PIPELINE_ROOT,\n",
    "    os.path.dirname(METADATA_PATH),\n",
    "    SERVING_MODEL_DIR,\n",
    "    DATA_ROOT,\n",
    "    MODULES_DIR\n",
    "]:\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "# Lokasi modul pipeline\n",
    "TRANSFORM_MODULE_FILE = os.path.join(MODULES_DIR, \"preprocessing.py\")\n",
    "TRAINER_MODULE_FILE = os.path.join(MODULES_DIR, \"trainer.py\")\n",
    "TUNER_MODULE_FILE = os.path.join(MODULES_DIR, \"tuner.py\")\n",
    "\n",
    "# Informasi konfigurasi\n",
    "print(\"PIPELINE_ROOT     :\", PIPELINE_ROOT)\n",
    "print(\"METADATA_PATH     :\", METADATA_PATH)\n",
    "print(\"SERVING_MODEL_DIR :\", SERVING_MODEL_DIR)\n",
    "print(\"DATA_ROOT         :\", DATA_ROOT)\n",
    "print(\"MODULES_DIR       :\", MODULES_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caa8a7c",
   "metadata": {},
   "source": [
    "Pada bagian ini, saya menentukan seluruh variabel dasar yang akan digunakan sebagai fondasi struktur pipeline. Saya memulai dengan menetapkan direktori kerja utama, kemudian mendefinisikan nama pipeline yang akan dibangun serta nama pipeline terpisah yang digunakan khusus untuk proses pembuatan schema. Setelah itu, saya menyusun struktur direktori artefak seperti lokasi penyimpanan pipeline, metadata, model hasil training, data mentah, dan modul Python yang digunakan dalam proses preprocessing, training, dan tuning.\n",
    "\n",
    "Agar pipeline dapat berjalan tanpa hambatan, seluruh direktori penting saya pastikan sudah tersedia dengan membuatnya secara otomatis apabila belum ada. Selanjutnya, saya menetapkan path lengkap untuk modul-modul kustom yang akan digunakan TFX dalam menjalankan transformasi data, proses pelatihan model, dan tuning hyperparameter. Pada akhir bagian ini, saya menampilkan seluruh konfigurasi utama untuk memastikan bahwa lokasi penyimpanan artefak sudah benar sebelum pipeline dijalankan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c74127",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824e5f1f",
   "metadata": {},
   "source": [
    "### ExampleGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6ef456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# konfigurasi pembagian data: 80% train, 20% eval (8:2)\n",
    "output_config = example_gen_pb2.Output(\n",
    "    split_config = example_gen_pb2.SplitConfig(splits=[\n",
    "        example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=8),\n",
    "        example_gen_pb2.SplitConfig.Split(name='eval',  hash_buckets=2),\n",
    "    ])\n",
    ")\n",
    "\n",
    "# komponen ExampleGen untuk membaca data CSV di folder DATA_ROOT\n",
    "example_gen = CsvExampleGen(\n",
    "    input_base=DATA_ROOT,\n",
    "    output_config=output_config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb81480e",
   "metadata": {},
   "source": [
    "Pada tahap ini, saya menyiapkan proses data ingestion menggunakan komponen ExampleGen, yaitu komponen TFX yang bertanggung jawab membaca data mentah dan mengubahnya menjadi format TFRecord yang siap digunakan oleh pipeline. Saya terlebih dahulu menentukan konfigurasi pembagian data, yaitu 80% untuk pelatihan dan 20% untuk evaluasi, dengan memanfaatkan mekanisme hash bucket agar proses pemisahan tetap konsisten di setiap eksekusi.\n",
    "\n",
    "Setelah konfigurasi pemisahan dibuat, saya menginisialisasi CsvExampleGen untuk membaca seluruh file CSV yang berada di direktori data. Komponen ini akan menghasilkan artefak Examples yang berisi data terstruktur untuk tahap-tahap berikutnya seperti pembuatan statistik, schema, dan transformasi. Dengan demikian, proses ingestion berjalan otomatis dan tetap terkontrol di dalam pipeline TFX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3c227d",
   "metadata": {},
   "source": [
    "## Data Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d0704b",
   "metadata": {},
   "source": [
    "### StatisticsGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73fc628b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Komponen StatisticsGen untuk menghitung statistik data\n",
    "statistics_gen = StatisticsGen(\n",
    "    examples=example_gen.outputs['examples']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442586cd",
   "metadata": {},
   "source": [
    "Pada tahap ini, saya menggunakan komponen StatisticsGen untuk menghasilkan statistik deskriptif dari data yang telah diproses oleh ExampleGen. Komponen ini menghitung berbagai metrik penting seperti distribusi nilai, frekuensi, rerata, hingga deteksi nilai yang hilang. Statistik tersebut menjadi dasar yang sangat penting untuk memahami karakteristik data dan akan digunakan oleh komponen lain, seperti SchemaGen dan ExampleValidator, dalam proses validasi. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e96fd0",
   "metadata": {},
   "source": [
    "### SchemaGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c04e4e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Komponen SchemaGen untuk menghasilkan schema data\n",
    "schema_gen = SchemaGen(\n",
    "    statistics=statistics_gen.outputs['statistics'],\n",
    "    infer_feature_shape=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6feed1d1",
   "metadata": {},
   "source": [
    "Pada bagian ini, saya menggunakan komponen SchemaGen untuk menghasilkan schema otomatis berdasarkan statistik yang telah dibuat sebelumnya. Schema ini berisi definisi struktural dari data, seperti tipe fitur, rentang nilai yang diharapkan, serta apakah suatu fitur bersifat wajib atau opsional. Dengan mengaktifkan infer_feature_shape=True, saya meminta TFX untuk turut menafsirkan bentuk atau dimensi fitur jika memungkinkan. Hasil schema ini menjadi acuan penting bagi pipeline, terutama untuk mendeteksi anomali pada data baru dan memastikan proses preprocessing serta training berjalan dengan konsisten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66151c9f",
   "metadata": {},
   "source": [
    "### ExampleValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2504554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Komponen ExampleValidator untuk memvalidasi data berdasarkan schema\n",
    "example_validator = ExampleValidator(\n",
    "    statistics=statistics_gen.outputs['statistics'],\n",
    "    schema=schema_gen.outputs['schema']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcd962b",
   "metadata": {},
   "source": [
    "Pada tahap ini, saya menggunakan komponen ExampleValidator untuk memeriksa kualitas data dengan membandingkannya terhadap schema yang telah dihasilkan sebelumnya. Komponen ini membantu mendeteksi berbagai anomali seperti nilai yang berada di luar rentang wajar, fitur yang hilang, atau ketidaksesuaian tipe data. Dengan melakukan validasi ini sejak awal, saya dapat memastikan bahwa data yang masuk ke tahap preprocessing dan training memiliki kualitas yang baik dan konsisten, sehingga potensi error atau bias selama pelatihan model dapat diminimalkan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01b994b",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c27c63",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5979452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing d:\\dicoding\\Submission_MLOps_2\\mrickyr-notebook\\modules\\preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRANSFORM_MODULE_FILE}\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "# fitur numerik\n",
    "NUMERIC_FEATURE_KEYS = [\n",
    "    'Age',\n",
    "    'BMI',\n",
    "    'BloodPressure',\n",
    "    'DiabetesPedigreeFunction',\n",
    "    'Glucose',\n",
    "    'Insulin',\n",
    "    'Pregnancies',\n",
    "    'SkinThickness'\n",
    "]\n",
    "\n",
    "# label (target)\n",
    "LABEL_KEY = 'Outcome'\n",
    "\n",
    "# fitur bernilai 0 tidak masuk akal (anggap saja missing)\n",
    "ZERO_AS_MISSING = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
    "\n",
    "def _xf(name): \n",
    "    '''Tambahkan suffix _xf pada nama fitur hasil transformasi.'''\n",
    "    return f\"{name}_xf\"\n",
    "\n",
    "def _impute_zero_with_nonzero_mean(x: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Mengganti nilai nol dengan rata-rata dari nilai bukan nol menggunakan analyzer TFT. \n",
    "    ...\n",
    "    \"\"\"\n",
    "    x = tf.cast(x, tf.float32)\n",
    "    x = tf.where(tf.math.is_nan(x), tf.zeros_like(x), x)\n",
    "\n",
    "    is_zero = tf.equal(x, 0.0)\n",
    "    masked_sum = tf.where(is_zero, tf.zeros_like(x), x)\n",
    "\n",
    "    sum_all = tft.mean(masked_sum)\n",
    "    zero_ratio = tft.mean(tf.cast(is_zero, tf.float32))\n",
    "    nonzero_ratio = 1.0 - zero_ratio\n",
    "    mean_nonzero = sum_all / tf.maximum(nonzero_ratio, 1e-6)\n",
    "\n",
    "    return tf.where(is_zero, mean_nonzero, x)\n",
    "\n",
    "\n",
    "def preprocessing_fn(inputs):\n",
    "    \"\"\"\n",
    "    Fungsi preprocessing...\n",
    "    \"\"\"\n",
    "    outputs = {}\n",
    "\n",
    "    for key in NUMERIC_FEATURE_KEYS:\n",
    "        val = tf.cast(inputs[key], tf.float32)\n",
    "        val = tf.where(tf.math.is_nan(val), tf.zeros_like(val), val)\n",
    "\n",
    "        if key in ZERO_AS_MISSING:\n",
    "            val = _impute_zero_with_nonzero_mean(val)\n",
    "\n",
    "        # Standardisasi\n",
    "        outputs[_xf(key)] = tft.scale_to_z_score(val)\n",
    "\n",
    "    # Label wajib int64\n",
    "    outputs[_xf(LABEL_KEY)] = tf.cast(inputs[LABEL_KEY], tf.int64)\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fb5da9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# komponen Transform untuk melakukan preprocessing data\n",
    "transform = Transform(\n",
    "    examples=example_gen.outputs['examples'],\n",
    "    schema=schema_gen.outputs['schema'],\n",
    "    module_file=TRANSFORM_MODULE_FILE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ef4176",
   "metadata": {},
   "source": [
    "Pada bagian ini, saya mendefinisikan modul preprocessing yang akan digunakan oleh komponen Transform dalam TFX. Saya terlebih dahulu menentukan daftar fitur numerik yang akan diproses, label target, serta fitur-fitur yang memiliki nilai nol namun secara konteks dianggap tidak logis dan diperlakukan sebagai nilai hilang, seperti pada kolom Glucose, BloodPressure, SkinThickness, Insulin, dan BMI. Untuk menangani hal tersebut, saya membuat fungsi khusus _impute_zero_with_nonzero_mean yang menggunakan analyzer dari tensorflow_transform untuk menggantikan nilai nol dengan rata-rata dari nilai bukan nol, sehingga distribusi data menjadi lebih masuk akal secara statistik. Seluruh fitur numerik kemudian distandardisasi menggunakan tft.scale_to_z_score, sedangkan label Outcome dikonversi ke tipe int64 agar konsisten dengan kebutuhan tahap training.\n",
    "\n",
    "Setelah fungsi preprocessing_fn didefinisikan, saya menghubungkannya ke dalam komponen Transform dengan memasukkan artefak Examples dan Schema sebagai input serta module_file sebagai lokasi modul preprocessing. Komponen Transform ini akan mengeksekusi seluruh logika preprocessing secara terkontrol di dalam pipeline, menghasilkan fitur-fitur yang sudah dibersihkan dan ditransformasi, sekaligus menyimpan graf transformasi yang nantinya dapat digunakan kembali saat serving model di lingkungan produksi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91097467",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659d1058",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cfc44d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing d:\\dicoding\\Submission_MLOps_2\\mrickyr-notebook\\modules\\trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINER_MODULE_FILE}\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "from tensorflow.keras import layers\n",
    "from tfx.components.trainer.fn_args_utils import FnArgs\n",
    "\n",
    "# Konstanta dan Helper Function\n",
    "LABEL_KEY = \"Outcome\"  # label asli\n",
    "\n",
    "\n",
    "def transformed_name(key: str) -> str:\n",
    "    \"\"\"Menambahkan suffix '_xf' untuk menandai fitur hasil transformasi.\"\"\"\n",
    "    return key + \"_xf\"\n",
    "\n",
    "\n",
    "def gzip_reader_fn(filenames):\n",
    "    \"\"\"Membaca TFRecord hasil transformasi (kompresi GZIP).\"\"\"\n",
    "    return tf.data.TFRecordDataset(filenames, compression_type='GZIP')\n",
    "\n",
    "\n",
    "# Input Function\n",
    "def input_fn(file_pattern, tf_transform_output, num_epochs=None, batch_size=64) -> tf.data.Dataset:\n",
    "    \"\"\"\n",
    "    Membuat dataset TF dari file TFRecord hasil transformasi.\n",
    "\n",
    "    Args:\n",
    "        file_pattern (str): Pola path file TFRecord hasil transformasi.\n",
    "        tf_transform_output (tft.TFTransformOutput): Objek hasil komponen Transform.\n",
    "        num_epochs (int): Jumlah epoch untuk membaca data (None = infinite).\n",
    "        batch_size (int): Ukuran batch.\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: Dataset yang siap digunakan untuk training/evaluasi.\n",
    "    \"\"\"\n",
    "    # Mendapatkan spesifikasi fitur hasil transformasi\n",
    "    transform_feature_spec = tf_transform_output.transformed_feature_spec().copy()\n",
    "\n",
    "    # Membuat dataset dalam bentuk batch\n",
    "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "        file_pattern=file_pattern,\n",
    "        batch_size=batch_size,\n",
    "        features=transform_feature_spec,\n",
    "        reader=gzip_reader_fn,\n",
    "        num_epochs=num_epochs,\n",
    "        label_key=transformed_name(LABEL_KEY)\n",
    "    ).repeat()\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def model_builder(hparams=None):\n",
    "    \"\"\"\n",
    "    Membangun model jaringan saraf tiruan (MLP) sederhana untuk klasifikasi biner.\n",
    "\n",
    "    Args:\n",
    "        hparams (dict, optional): Hyperparameter dari komponen Tuner (jika ada).\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: Model Keras yang sudah dikompilasi.\n",
    "    \"\"\"\n",
    "    # deteksi tipe parameter (dict atau HyperParameters)\n",
    "    if hparams is None:\n",
    "        # jika dipanggil tanpa tuner (Trainer biasa)\n",
    "        hidden_units = 64\n",
    "        dropout_rate = 0.2\n",
    "        learning_rate = 1e-3\n",
    "\n",
    "    elif hasattr(hparams, \"Choice\"):  # dipanggil oleh keras_tuner\n",
    "        hp = hparams\n",
    "        hidden_units = hp.Int(\"units\", min_value=32, max_value=128, step=32)\n",
    "        dropout_rate = hp.Float(\"dropout\", min_value=0.1, max_value=0.5, step=0.1)\n",
    "        # turunkan range learning rate biar tidak terlalu agresif\n",
    "        learning_rate = hp.Choice(\"learning_rate\", [1e-3, 5e-4, 1e-4])\n",
    "\n",
    "    else:  # dipanggil oleh Trainer dengan dict hasil dari tuner_fn\n",
    "        hidden_units = int(hparams.get(\"units\", 64))\n",
    "        dropout_rate = float(hparams.get(\"dropout\", 0.2))\n",
    "        learning_rate = float(hparams.get(\"learning_rate\", 1e-3))\n",
    "\n",
    "    # definisi input layer sesuai fitur hasil Transform\n",
    "    inputs = {\n",
    "        name: layers.Input(shape=(1,), name=name, dtype=tf.float32)\n",
    "        for name in [\n",
    "            'Age_xf', 'BMI_xf', 'BloodPressure_xf', 'DiabetesPedigreeFunction_xf',\n",
    "            'Glucose_xf', 'Insulin_xf', 'Pregnancies_xf', 'SkinThickness_xf'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Concatenate seluruh fitur numerik\n",
    "    x = layers.Concatenate(name=\"concatenate_inputs\")(list(inputs.values()))\n",
    "    \n",
    "    # hidden layers\n",
    "    x = layers.Dense(hidden_units, activation='relu')(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(hidden_units // 2, activation='relu')(x)\n",
    "\n",
    "    # output layer\n",
    "    outputs = layers.Dense(1, activation='sigmoid', name='probability')(x)\n",
    "\n",
    "    # compile model\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[\n",
    "            tf.keras.metrics.BinaryAccuracy(name='binary_accuracy'),\n",
    "            tf.keras.metrics.AUC(name='auc'),\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall')\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # tampilkan ringkasan model\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "# Serving Function (untuk model deployment)\n",
    "def _get_serve_tf_examples_fn(model, tf_transform_output):\n",
    "    \"\"\"\n",
    "    Mendefinisikan signature function agar model dapat menerima input mentah\n",
    "    dalam format tf.Example saat deployment (TensorFlow Serving).\n",
    "    \"\"\"\n",
    "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
    "\n",
    "    @tf.function\n",
    "    def serve_tf_examples_fn(serialized_tf_examples):\n",
    "        feature_spec = tf_transform_output.raw_feature_spec()\n",
    "        feature_spec.pop(LABEL_KEY)\n",
    "        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
    "        transformed_features = model.tft_layer(parsed_features)\n",
    "        return model(transformed_features)\n",
    "\n",
    "    return serve_tf_examples_fn\n",
    "\n",
    "\n",
    "def run_fn(fn_args: FnArgs) -> None:\n",
    "    \"\"\"\n",
    "    Fungsi utama untuk melatih model.\n",
    "    Fungsi ini dijalankan oleh komponen Trainer TFX dan meliputi:\n",
    "    - Membaca hasil transformasi,\n",
    "    - Menyusun pipeline input,\n",
    "    - Melatih model dengan callback,\n",
    "    - Menyimpan model beserta serving signature.\n",
    "    \"\"\"\n",
    "    # Muat hasil Transform\n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
    "\n",
    "    # Siapkan dataset training & evaluasi\n",
    "    train_set = input_fn(fn_args.train_files, tf_transform_output, num_epochs=10)\n",
    "    eval_set  = input_fn(fn_args.eval_files,  tf_transform_output, num_epochs=10)\n",
    "\n",
    "    # Ambil hyperparameter jika Tuner digunakan\n",
    "    hparams = None\n",
    "    if getattr(fn_args, \"hyperparameters\", None):\n",
    "        try:\n",
    "            hparams = fn_args.hyperparameters.get(\"values\") or fn_args.hyperparameters\n",
    "        except Exception:\n",
    "            hparams = None\n",
    "\n",
    "    # Bangun model\n",
    "    model = model_builder(hparams=hparams)\n",
    "\n",
    "    # Siapkan callback\n",
    "    log_dir = os.path.join(os.path.dirname(fn_args.serving_model_dir), 'logs')\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, update_freq='epoch')\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_auc', mode='max', patience=10, restore_best_weights=True, verbose=1)\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_auc', mode='max', factor=0.5, patience=4, min_lr=1e-5, verbose=1)\n",
    "\n",
    "    # Latih model\n",
    "    model.fit(\n",
    "        x=train_set,\n",
    "        validation_data=eval_set,\n",
    "        steps_per_epoch=fn_args.train_steps,\n",
    "        validation_steps=fn_args.eval_steps,\n",
    "        epochs=50,\n",
    "        callbacks=[tensorboard_callback, early_stopping, reduce_lr],\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    # Buat serving signature\n",
    "    signatures = {\n",
    "        'serving_default':\n",
    "        _get_serve_tf_examples_fn(model, tf_transform_output).get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # Simpan model dalam format TensorFlow SavedModel\n",
    "    model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b251ee04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# komponen Trainer untuk melatih model\n",
    "trainer = Trainer(\n",
    "    module_file=TRAINER_MODULE_FILE,\n",
    "    examples=transform.outputs['transformed_examples'],\n",
    "    transform_graph=transform.outputs['transform_graph'],\n",
    "    schema=schema_gen.outputs['schema'],\n",
    "    # train/eval steps bisa kamu naikkan nanti setelah semuanya jalan\n",
    "    train_args=trainer_pb2.TrainArgs(splits=['train'], num_steps=10),\n",
    "    eval_args=trainer_pb2.EvalArgs(splits=['eval'], num_steps=3),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2461edae",
   "metadata": {},
   "source": [
    "Pada bagian ini, saya mendefinisikan seluruh logika pelatihan model yang akan dijalankan oleh komponen Trainer di dalam pipeline TFX. Saya mulai dengan membuat fungsi input_fn yang bertugas membaca data hasil Transform dalam format TFRecord terkompresi GZIP dan mengubahnya menjadi tf.data.Dataset yang siap digunakan untuk proses training dan evaluasi. Selanjutnya, saya membangun sebuah model jaringan saraf tiruan sederhana (MLP) melalui fungsi model_builder, dengan arsitektur yang memanfaatkan fitur-fitur numerik yang sudah ditransformasi, lapisan dense bertingkat, dropout untuk mencegah overfitting, serta output sigmoid untuk menangani kasus klasifikasi biner. Fungsi ini juga saya rancang agar fleksibel terhadap penggunaan hyperparameter, baik ketika dipanggil secara standar maupun terintegrasi dengan komponen Tuner.\n",
    "\n",
    "Untuk keperluan deployment, saya menyiapkan fungsi _get_serve_tf_examples_fn yang mendefinisikan serving signature sehingga model dapat menerima input mentah dalam bentuk tf.Example dan menerapkan graf transformasi yang sama seperti saat training. Seluruh proses ini kemudian diorkestrasi di dalam run_fn, yang dijalankan oleh komponen Trainer: mulai dari memuat hasil Transform, menyiapkan dataset train dan eval, membangun model, hingga menjalankan pelatihan dengan callback seperti TensorBoard, early stopping, dan penyesuaian learning rate berbasis performa validasi. Pada akhir fungsi, model disimpan dalam format SavedModel lengkap dengan signature untuk serving. Terakhir, saya mengonfigurasi komponen Trainer di pipeline dengan menghubungkannya ke artefak transformed_examples, transform_graph, dan schema, serta menetapkan jumlah langkah training dan evaluasi awal agar proses pelatihan dapat berjalan otomatis di dalam pipeline TFX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad653a6",
   "metadata": {},
   "source": [
    "### Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a3ab83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing d:\\dicoding\\Submission_MLOps_2\\mrickyr-notebook\\modules\\tuner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TUNER_MODULE_FILE}\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import keras_tuner as kt\n",
    "from tfx.components.trainer.fn_args_utils import FnArgs\n",
    "from tfx.components.tuner.component import TunerFnResult\n",
    "\n",
    "# Import fungsi dari trainer module\n",
    "from trainer import input_fn, model_builder, transformed_name, LABEL_KEY\n",
    "\n",
    "\n",
    "def tuner_fn(fn_args: FnArgs) -> TunerFnResult:\n",
    "    \"\"\"\n",
    "    Fungsi utama untuk menjalankan hyperparameter tuning.\n",
    "    Menggunakan model_builder dan input_fn dari trainer.py.\n",
    "    \"\"\"\n",
    "    # Muat hasil Transform\n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
    "\n",
    "    # Siapkan dataset training & evaluasi (pakai fungsi dari trainer.py)\n",
    "    train_set = input_fn(fn_args.train_files, tf_transform_output, num_epochs=10)\n",
    "    val_set   = input_fn(fn_args.eval_files,  tf_transform_output, num_epochs=10)\n",
    "\n",
    "    # Callback early stopping\n",
    "    stop_early = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_auc\", mode=\"max\", patience=10, restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # callback reduce learning rate\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_auc\", mode=\"max\", factor=0.5, patience=4, min_lr=1e-5, verbose=1\n",
    "    )\n",
    "\n",
    "    # Definisikan strategi tuning dengan Hyperband\n",
    "    tuner = kt.Hyperband(\n",
    "        model_builder,  # panggil fungsi model_builder dari trainer\n",
    "        objective=kt.Objective(\"val_auc\", direction=\"max\"),\n",
    "        max_epochs=20,\n",
    "        factor=3,\n",
    "        directory=fn_args.working_dir,\n",
    "        project_name=\"keras_tuner_linked\"\n",
    "    )\n",
    "\n",
    "    # Kembalikan hasil untuk komponen TFX Tuner\n",
    "    return TunerFnResult(\n",
    "        tuner=tuner,\n",
    "        fit_kwargs={\n",
    "            \"x\": train_set,\n",
    "            \"validation_data\": val_set,\n",
    "            \"steps_per_epoch\": fn_args.train_steps,\n",
    "            \"validation_steps\": fn_args.eval_steps,\n",
    "            \"epochs\": 50,\n",
    "            \"callbacks\": [stop_early, reduce_lr],\n",
    "            \"verbose\": 2\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bd0ba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# komponen Tuner untuk hyperparameter tuning\n",
    "tuner = Tuner(\n",
    "    module_file=TUNER_MODULE_FILE,\n",
    "    examples=transform.outputs['transformed_examples'],\n",
    "    transform_graph=transform.outputs['transform_graph'],\n",
    "    schema=schema_gen.outputs['schema'],\n",
    "    train_args=trainer_pb2.TrainArgs(splits=['train'], num_steps=10),\n",
    "    eval_args=trainer_pb2.EvalArgs(splits=['eval'], num_steps=3),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ba52ba",
   "metadata": {},
   "source": [
    "Pada bagian ini, saya menyiapkan proses hyperparameter tuning dengan memanfaatkan komponen Tuner di TFX yang terintegrasi dengan Keras Tuner. Saya menggunakan kembali fungsi input_fn dan model_builder dari modul trainer.py agar arsitektur model dan alur input yang digunakan saat tuning konsisten dengan proses training utama. Melalui fungsi tuner_fn, saya terlebih dahulu memuat hasil Transform, lalu menyusun dataset train dan validasi dari artefak yang sama seperti yang digunakan Trainer.\n",
    "\n",
    "Saya kemudian mendefinisikan dua callback penting, yaitu early stopping dan reduce learning rate, yang membantu menghentikan eksperimen lebih awal ketika performa tidak lagi membaik serta menyesuaikan laju pembelajaran secara adaptif. Untuk strategi pencarian hyperparameter, saya menggunakan Hyperband dengan tujuan mengoptimalkan metrik val_auc. Objek tuner beserta parameter pemanggilan fit dikembalikan dalam bentuk TunerFnResult, sehingga dapat dieksekusi secara otomatis oleh komponen TFX Tuner. Terakhir, saya mengonfigurasi komponen Tuner di pipeline dengan menghubungkannya ke artefak transformed_examples, transform_graph, dan schema, serta mendefinisikan jumlah langkah train dan eval. Dengan cara ini, proses pencarian kombinasi hyperparameter terbaik berjalan terstruktur dan tetap terintegrasi penuh di dalam pipeline TFX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fadc786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer dengan hyperparameter terbaik dari Tuner\n",
    "trainer = Trainer(\n",
    "    module_file=TRAINER_MODULE_FILE,\n",
    "    examples=transform.outputs['transformed_examples'],\n",
    "    transform_graph=transform.outputs['transform_graph'],\n",
    "    schema=schema_gen.outputs['schema'],\n",
    "    hyperparameters=tuner.outputs['best_hyperparameters'],\n",
    "    train_args=trainer_pb2.TrainArgs(splits=['train'], num_steps=10),\n",
    "    eval_args=trainer_pb2.EvalArgs(splits=['eval'], num_steps=3),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3de583",
   "metadata": {},
   "source": [
    "Pada tahap ini, saya menjalankan kembali komponen Trainer dengan memanfaatkan hyperparameter terbaik yang diperoleh dari komponen Tuner. Alih-alih menggunakan nilai default, Trainer kini menerima artefak best_hyperparameters sebagai input sehingga arsitektur dan konfigurasi model yang dilatih sudah dioptimalkan berdasarkan proses pencarian sebelumnya. Saya tetap menggunakan hasil transformasi (transformed_examples dan transform_graph) serta schema yang sama agar alur data dan definisi fitur tetap konsisten. Dengan cara ini, proses retraining dilakukan secara lebih terarah karena model dilatih ulang menggunakan kombinasi hyperparameter yang sudah terbukti memberikan performa terbaik pada data validasi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a720718",
   "metadata": {},
   "source": [
    "## Model Analysis and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929db68d",
   "metadata": {},
   "source": [
    "### Resolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2a79a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolver component\n",
    "\n",
    "model_resolver = Resolver(\n",
    "    strategy_class=LatestBlessedModelStrategy,\n",
    "    model=Channel(type=Model),\n",
    "    model_blessing=Channel(type=ModelBlessing),\n",
    ").with_id(\"latest_blessed_model_resolver\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc71e54",
   "metadata": {},
   "source": [
    "Pada bagian ini, saya menambahkan komponen Resolver untuk memilih model terbaik yang telah lolos proses evaluasi pada eksekusi sebelumnya. Dengan menggunakan strategi LatestBlessedModelStrategy, komponen ini secara otomatis mencari artefak model paling mutakhir yang telah menerima status “blessed”, yakni model yang dinilai memenuhi kriteria performa oleh komponen Evaluator. Informasi ini kemudian digunakan oleh tahap pushing agar hanya model yang sudah tervalidasi yang diteruskan ke proses deployment. Dengan menambahkan Resolver, pipeline menjadi lebih aman dan stabil karena mencegah model yang kurang baik atau belum diverifikasi masuk ke tahap produksi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ba47bb",
   "metadata": {},
   "source": [
    "### Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2cd2179d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# konfigurasi Evaluator\n",
    "eval_config = tfma.EvalConfig(\n",
    "    model_specs=[tfma.ModelSpec(label_key='Outcome')],\n",
    "    slicing_specs=[tfma.SlicingSpec()],  # overall\n",
    "    metrics_specs=[\n",
    "        tfma.MetricsSpec(metrics=[\n",
    "            tfma.MetricConfig(class_name='ExampleCount'),\n",
    "\n",
    "            # kemampuan diskriminasi menyeluruh\n",
    "            tfma.MetricConfig(\n",
    "                class_name='AUC',\n",
    "                threshold=tfma.MetricThreshold(\n",
    "                    value_threshold=tfma.GenericValueThreshold(\n",
    "                        lower_bound={'value': 0.80}\n",
    "                    ),\n",
    "                    change_threshold=tfma.GenericChangeThreshold(\n",
    "                        direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
    "                        absolute={'value': -0.01}\n",
    "                    )\n",
    "                )\n",
    "            ),\n",
    "\n",
    "            # minimalkan false negative\n",
    "            tfma.MetricConfig(\n",
    "                class_name='Recall',\n",
    "                threshold=tfma.MetricThreshold(\n",
    "                    value_threshold=tfma.GenericValueThreshold(\n",
    "                        lower_bound={'value': 0.50}\n",
    "                    ),\n",
    "                    change_threshold=tfma.GenericChangeThreshold(\n",
    "                        direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
    "                        absolute={'value': -0.01}\n",
    "                    )\n",
    "                )\n",
    "            ),\n",
    "\n",
    "            # Precision tetap dijaga agar tidak terlalu banyak false positive\n",
    "            tfma.MetricConfig(\n",
    "                class_name='Precision',\n",
    "                threshold=tfma.MetricThreshold(\n",
    "                    value_threshold=tfma.GenericValueThreshold(\n",
    "                        lower_bound={'value': 0.50}\n",
    "                    ),\n",
    "                    change_threshold=tfma.GenericChangeThreshold(\n",
    "                        direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
    "                        absolute={'value': -0.01}\n",
    "                    )\n",
    "                )\n",
    "            ),\n",
    "\n",
    "            # Akurasi sebagai sanity check\n",
    "            tfma.MetricConfig(\n",
    "                class_name='BinaryAccuracy',\n",
    "                threshold=tfma.MetricThreshold(\n",
    "                    value_threshold=tfma.GenericValueThreshold(\n",
    "                        lower_bound={'value': 0.70}\n",
    "                    ),\n",
    "                    change_threshold=tfma.GenericChangeThreshold(\n",
    "                        direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
    "                        absolute={'value': -0.01}\n",
    "                    )\n",
    "                )\n",
    "            ),\n",
    "\n",
    "            # Komponen confusion matrix (tanpa threshold)\n",
    "            tfma.MetricConfig(class_name='TruePositives'),\n",
    "            tfma.MetricConfig(class_name='FalsePositives'),\n",
    "            tfma.MetricConfig(class_name='TrueNegatives'),\n",
    "            tfma.MetricConfig(class_name='FalseNegatives')\n",
    "        ])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3651c3",
   "metadata": {},
   "source": [
    "Pada bagian ini, saya menyusun konfigurasi TFMA Evaluator untuk menilai kualitas model secara lebih terarah sesuai tujuan proyek, yaitu meminimalkan kasus pasien yang sebenarnya mengidap diabetes tetapi diprediksi tidak (false negative). Saya mendefinisikan EvalConfig dengan label Outcome sebagai target, lalu menambahkan beberapa metrik utama. Metrik AUC digunakan untuk mengukur kemampuan model membedakan kelas sehat dan diabetes secara keseluruhan, dengan batas minimal 0,80. Untuk mengendalikan false negative, saya memberi perhatian khusus pada metrik Recall dengan ambang minimal 0,50, sehingga model diharapkan cukup sensitif dalam menangkap kasus diabetes. Di sisi lain, Precision juga dijaga di atas 0,50 agar jumlah prediksi positif yang keliru (false positive) tidak terlalu banyak. BinaryAccuracy ditambahkan sebagai sanity check dengan batas minimal 0,70 untuk memastikan performa global tetap wajar.\n",
    "\n",
    "Selain itu, saya ikut merekam komponen-komponen confusion matrix seperti True Positives, False Positives, True Negatives, dan False Negatives tanpa threshold tambahan. Informasi ini membantu saya menganalisis secara lebih rinci pola kesalahan model, terutama untuk memastikan bahwa jumlah false negative benar-benar dapat ditekan sesuai dengan tujuan bisnis dan konteks medis dari kasus diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b5a713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluator component (untuk Apache Beam pipeline)\n",
    "evaluator = Evaluator(\n",
    "    examples=example_gen.outputs['examples'],\n",
    "    model=trainer.outputs['model'],\n",
    "    baseline_model=model_resolver.outputs['model'],  # boleh kosong di run pertama\n",
    "    eval_config=eval_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a482b9",
   "metadata": {},
   "source": [
    "Pada tahap ini, saya mengaktifkan komponen Evaluator untuk menjalankan evaluasi model menggunakan Apache Beam dan konfigurasi metrik yang telah ditetapkan sebelumnya. Komponen ini menerima artefak examples sebagai data evaluasi dan model hasil training sebagai objek yang akan dianalisis. Jika tersedia, saya juga memasukkan baseline_model dari Resolver, sehingga Evaluator dapat membandingkan performa model baru terhadap model terbaik yang pernah dipromosikan sebelumnya. Dengan cara ini, pipeline dapat memastikan bahwa model baru tidak hanya memenuhi ambang kinerja yang ditentukan, tetapi juga tidak mengalami penurunan performa dibandingkan baseline. Hasil evaluasi inilah yang nantinya menentukan apakah model baru layak diberi status “blessed” dan diteruskan ke tahap deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4c2d7d",
   "metadata": {},
   "source": [
    "## Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3a2131",
   "metadata": {},
   "source": [
    "### Pusher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d1fd0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pusher component\n",
    "pusher = Pusher(\n",
    "    model=trainer.outputs[\"model\"],\n",
    "    model_blessing=evaluator.outputs[\"blessing\"],  # hanya push kalau model \"blessed\"\n",
    "    push_destination=pusher_pb2.PushDestination(\n",
    "        filesystem=pusher_pb2.PushDestination.Filesystem(\n",
    "            base_directory=SERVING_MODEL_DIR\n",
    "        )\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec3d965",
   "metadata": {},
   "source": [
    "Pada bagian ini, saya menambahkan komponen Pusher yang bertugas melakukan proses deployment model ke direktori tujuan. Komponen ini hanya akan mengekspor model apabila artefak blessing dari Evaluator menyatakan bahwa model layak digunakan, sehingga hanya model yang memenuhi ambang performa dan tidak mengalami penurunan kualitas yang dapat diteruskan ke tahap produksi. Saya menentukan lokasi penyimpanan menggunakan PushDestination berbasis filesystem, yaitu pada direktori SERVING_MODEL_DIR. Ketika sebuah model dinyatakan “blessed”, Pusher akan menyimpan versi terbaru dalam format SavedModel, sehingga model tersebut siap digunakan untuk inference atau integrasi dengan sistem serving seperti TensorFlow Serving. Dengan mekanisme ini, pipeline memastikan bahwa proses deployment berlangsung aman, terkontrol, dan sepenuhnya otomatis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b388ec8",
   "metadata": {},
   "source": [
    "## Run TFX Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0236a18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 00m 07s]\n",
      "val_auc: 0.8633618354797363\n",
      "\n",
      "Best val_auc So Far: 0.8740265965461731\n",
      "Total elapsed time: 00h 01m 50s\n",
      "Results summary\n",
      "Results in d:\\dicoding\\Submission_MLOps_2\\mrickyr-notebook\\pipelines\\mrickyr-pipeline\\Tuner\\.system\\executor_execution\\7\\.temp\\7\\keras_tuner_linked\n",
      "Showing 10 best trials\n",
      "<keras_tuner.engine.objective.Objective object at 0x000001A291214D30>\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 128\n",
      "dropout: 0.5\n",
      "learning_rate: 0.0005\n",
      "tuner/epochs: 20\n",
      "tuner/initial_epoch: 7\n",
      "tuner/bracket: 1\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0019\n",
      "Score: 0.8740265965461731\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 128\n",
      "dropout: 0.4\n",
      "learning_rate: 0.001\n",
      "tuner/epochs: 20\n",
      "tuner/initial_epoch: 7\n",
      "tuner/bracket: 2\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0014\n",
      "Score: 0.8714698553085327\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 96\n",
      "dropout: 0.4\n",
      "learning_rate: 0.001\n",
      "tuner/epochs: 20\n",
      "tuner/initial_epoch: 7\n",
      "tuner/bracket: 2\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0012\n",
      "Score: 0.8692817687988281\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 96\n",
      "dropout: 0.2\n",
      "learning_rate: 0.0005\n",
      "tuner/epochs: 20\n",
      "tuner/initial_epoch: 7\n",
      "tuner/bracket: 1\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0021\n",
      "Score: 0.8680341839790344\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 96\n",
      "dropout: 0.1\n",
      "learning_rate: 0.001\n",
      "tuner/epochs: 20\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 0\n",
      "tuner/round: 0\n",
      "Score: 0.8633618354797363\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 96\n",
      "dropout: 0.4\n",
      "learning_rate: 0.001\n",
      "tuner/epochs: 7\n",
      "tuner/initial_epoch: 3\n",
      "tuner/bracket: 2\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0009\n",
      "Score: 0.8586324453353882\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 96\n",
      "dropout: 0.2\n",
      "learning_rate: 0.0005\n",
      "tuner/epochs: 7\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.8585648536682129\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 96\n",
      "dropout: 0.4\n",
      "learning_rate: 0.001\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 2\n",
      "tuner/round: 0\n",
      "Score: 0.8564195036888123\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 64\n",
      "dropout: 0.5\n",
      "learning_rate: 0.001\n",
      "tuner/epochs: 20\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 0\n",
      "tuner/round: 0\n",
      "Score: 0.8552064299583435\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 96\n",
      "dropout: 0.2\n",
      "learning_rate: 0.001\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 2\n",
      "tuner/round: 0\n",
      "Score: 0.8525127172470093\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Age_xf (InputLayer)            [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " BMI_xf (InputLayer)            [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " BloodPressure_xf (InputLayer)  [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " DiabetesPedigreeFunction_xf (I  [(None, 1)]         0           []                               \n",
      " nputLayer)                                                                                       \n",
      "                                                                                                  \n",
      " Glucose_xf (InputLayer)        [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " Insulin_xf (InputLayer)        [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " Pregnancies_xf (InputLayer)    [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " SkinThickness_xf (InputLayer)  [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate_inputs (Concatenat  (None, 8)           0           ['Age_xf[0][0]',                 \n",
      " e)                                                               'BMI_xf[0][0]',                 \n",
      "                                                                  'BloodPressure_xf[0][0]',       \n",
      "                                                                  'DiabetesPedigreeFunction_xf[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'Glucose_xf[0][0]',             \n",
      "                                                                  'Insulin_xf[0][0]',             \n",
      "                                                                  'Pregnancies_xf[0][0]',         \n",
      "                                                                  'SkinThickness_xf[0][0]']       \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 128)          1152        ['concatenate_inputs[0][0]']     \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 128)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 64)           8256        ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " probability (Dense)            (None, 1)            65          ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9,473\n",
      "Trainable params: 9,473\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/50\n",
      "10/10 - 1s - loss: 0.6608 - binary_accuracy: 0.6016 - auc: 0.6869 - precision: 0.4377 - recall: 0.6729 - val_loss: 0.6042 - val_binary_accuracy: 0.7135 - val_auc: 0.7970 - val_precision: 0.6667 - val_recall: 0.5526 - lr: 5.0000e-04 - 1s/epoch - 123ms/step\n",
      "Epoch 2/50\n",
      "10/10 - 0s - loss: 0.5998 - binary_accuracy: 0.7094 - auc: 0.7490 - precision: 0.5885 - recall: 0.5136 - val_loss: 0.5661 - val_binary_accuracy: 0.7031 - val_auc: 0.8142 - val_precision: 0.7179 - val_recall: 0.3784 - lr: 5.0000e-04 - 185ms/epoch - 19ms/step\n",
      "Epoch 3/50\n",
      "10/10 - 0s - loss: 0.5565 - binary_accuracy: 0.7547 - auc: 0.7825 - precision: 0.7077 - recall: 0.4360 - val_loss: 0.5363 - val_binary_accuracy: 0.7448 - val_auc: 0.8273 - val_precision: 0.8333 - val_recall: 0.4110 - lr: 5.0000e-04 - 213ms/epoch - 21ms/step\n",
      "Epoch 4/50\n",
      "10/10 - 0s - loss: 0.5213 - binary_accuracy: 0.7469 - auc: 0.8142 - precision: 0.7177 - recall: 0.4120 - val_loss: 0.5244 - val_binary_accuracy: 0.7292 - val_auc: 0.8113 - val_precision: 0.7317 - val_recall: 0.4225 - lr: 5.0000e-04 - 237ms/epoch - 24ms/step\n",
      "Epoch 5/50\n",
      "10/10 - 0s - loss: 0.5041 - binary_accuracy: 0.7656 - auc: 0.8226 - precision: 0.7320 - recall: 0.5068 - val_loss: 0.4915 - val_binary_accuracy: 0.7500 - val_auc: 0.8459 - val_precision: 0.7727 - val_recall: 0.4722 - lr: 5.0000e-04 - 193ms/epoch - 19ms/step\n",
      "Epoch 6/50\n",
      "10/10 - 0s - loss: 0.5093 - binary_accuracy: 0.7484 - auc: 0.8025 - precision: 0.6857 - recall: 0.4507 - val_loss: 0.4891 - val_binary_accuracy: 0.7240 - val_auc: 0.8366 - val_precision: 0.7111 - val_recall: 0.4444 - lr: 5.0000e-04 - 207ms/epoch - 21ms/step\n",
      "Epoch 7/50\n",
      "10/10 - 0s - loss: 0.4950 - binary_accuracy: 0.7672 - auc: 0.8176 - precision: 0.7219 - recall: 0.5046 - val_loss: 0.4719 - val_binary_accuracy: 0.7240 - val_auc: 0.8482 - val_precision: 0.6800 - val_recall: 0.4789 - lr: 5.0000e-04 - 179ms/epoch - 18ms/step\n",
      "Epoch 8/50\n",
      "10/10 - 0s - loss: 0.4764 - binary_accuracy: 0.7703 - auc: 0.8313 - precision: 0.7006 - recall: 0.5688 - val_loss: 0.4800 - val_binary_accuracy: 0.7292 - val_auc: 0.8444 - val_precision: 0.7170 - val_recall: 0.5067 - lr: 5.0000e-04 - 199ms/epoch - 20ms/step\n",
      "Epoch 9/50\n",
      "10/10 - 0s - loss: 0.4696 - binary_accuracy: 0.7531 - auc: 0.8331 - precision: 0.6807 - recall: 0.5183 - val_loss: 0.4732 - val_binary_accuracy: 0.7240 - val_auc: 0.8451 - val_precision: 0.6909 - val_recall: 0.5135 - lr: 5.0000e-04 - 194ms/epoch - 19ms/step\n",
      "Epoch 10/50\n",
      "10/10 - 0s - loss: 0.5009 - binary_accuracy: 0.7565 - auc: 0.8073 - precision: 0.6890 - recall: 0.5355 - val_loss: 0.4584 - val_binary_accuracy: 0.7344 - val_auc: 0.8530 - val_precision: 0.6852 - val_recall: 0.5211 - lr: 5.0000e-04 - 203ms/epoch - 20ms/step\n",
      "Epoch 11/50\n",
      "10/10 - 0s - loss: 0.4449 - binary_accuracy: 0.7766 - auc: 0.8494 - precision: 0.6971 - recall: 0.5755 - val_loss: 0.4659 - val_binary_accuracy: 0.7344 - val_auc: 0.8544 - val_precision: 0.7308 - val_recall: 0.5067 - lr: 5.0000e-04 - 199ms/epoch - 20ms/step\n",
      "Epoch 12/50\n",
      "10/10 - 0s - loss: 0.4623 - binary_accuracy: 0.7594 - auc: 0.8378 - precision: 0.6765 - recall: 0.5374 - val_loss: 0.4569 - val_binary_accuracy: 0.7396 - val_auc: 0.8568 - val_precision: 0.7000 - val_recall: 0.5676 - lr: 5.0000e-04 - 190ms/epoch - 19ms/step\n",
      "Epoch 13/50\n",
      "10/10 - 0s - loss: 0.4672 - binary_accuracy: 0.7797 - auc: 0.8351 - precision: 0.7175 - recall: 0.5826 - val_loss: 0.4467 - val_binary_accuracy: 0.7500 - val_auc: 0.8634 - val_precision: 0.7069 - val_recall: 0.5694 - lr: 5.0000e-04 - 175ms/epoch - 17ms/step\n",
      "Epoch 14/50\n",
      "10/10 - 0s - loss: 0.4546 - binary_accuracy: 0.7672 - auc: 0.8422 - precision: 0.6839 - recall: 0.5587 - val_loss: 0.4523 - val_binary_accuracy: 0.7448 - val_auc: 0.8565 - val_precision: 0.7018 - val_recall: 0.5556 - lr: 5.0000e-04 - 187ms/epoch - 19ms/step\n",
      "Epoch 15/50\n",
      "10/10 - 0s - loss: 0.4635 - binary_accuracy: 0.7641 - auc: 0.8419 - precision: 0.6936 - recall: 0.5505 - val_loss: 0.4514 - val_binary_accuracy: 0.7552 - val_auc: 0.8623 - val_precision: 0.7414 - val_recall: 0.5733 - lr: 5.0000e-04 - 188ms/epoch - 19ms/step\n",
      "Epoch 16/50\n",
      "10/10 - 0s - loss: 0.4622 - binary_accuracy: 0.7750 - auc: 0.8425 - precision: 0.7143 - recall: 0.5708 - val_loss: 0.4374 - val_binary_accuracy: 0.7656 - val_auc: 0.8676 - val_precision: 0.7407 - val_recall: 0.5634 - lr: 5.0000e-04 - 210ms/epoch - 21ms/step\n",
      "Epoch 17/50\n",
      "10/10 - 0s - loss: 0.4546 - binary_accuracy: 0.7828 - auc: 0.8449 - precision: 0.7152 - recall: 0.5619 - val_loss: 0.4482 - val_binary_accuracy: 0.7552 - val_auc: 0.8595 - val_precision: 0.7241 - val_recall: 0.5753 - lr: 5.0000e-04 - 191ms/epoch - 19ms/step\n",
      "Epoch 18/50\n",
      "10/10 - 0s - loss: 0.4403 - binary_accuracy: 0.7703 - auc: 0.8554 - precision: 0.6774 - recall: 0.5915 - val_loss: 0.4477 - val_binary_accuracy: 0.7552 - val_auc: 0.8624 - val_precision: 0.7451 - val_recall: 0.5278 - lr: 5.0000e-04 - 187ms/epoch - 19ms/step\n",
      "Epoch 19/50\n",
      "10/10 - 0s - loss: 0.4654 - binary_accuracy: 0.7761 - auc: 0.8425 - precision: 0.7423 - recall: 0.5602 - val_loss: 0.4565 - val_binary_accuracy: 0.7656 - val_auc: 0.8568 - val_precision: 0.7636 - val_recall: 0.5676 - lr: 5.0000e-04 - 210ms/epoch - 21ms/step\n",
      "Epoch 20/50\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "10/10 - 0s - loss: 0.4704 - binary_accuracy: 0.7750 - auc: 0.8358 - precision: 0.7256 - recall: 0.5459 - val_loss: 0.4512 - val_binary_accuracy: 0.7604 - val_auc: 0.8606 - val_precision: 0.7455 - val_recall: 0.5616 - lr: 5.0000e-04 - 189ms/epoch - 19ms/step\n",
      "Epoch 21/50\n",
      "10/10 - 0s - loss: 0.4470 - binary_accuracy: 0.7719 - auc: 0.8483 - precision: 0.6763 - recall: 0.5652 - val_loss: 0.4363 - val_binary_accuracy: 0.7812 - val_auc: 0.8690 - val_precision: 0.7959 - val_recall: 0.5493 - lr: 2.5000e-04 - 204ms/epoch - 20ms/step\n",
      "Epoch 22/50\n",
      "10/10 - 0s - loss: 0.4548 - binary_accuracy: 0.7750 - auc: 0.8455 - precision: 0.7158 - recall: 0.5874 - val_loss: 0.4474 - val_binary_accuracy: 0.7708 - val_auc: 0.8580 - val_precision: 0.7647 - val_recall: 0.5493 - lr: 2.5000e-04 - 188ms/epoch - 19ms/step\n",
      "Epoch 23/50\n",
      "10/10 - 0s - loss: 0.4482 - binary_accuracy: 0.7844 - auc: 0.8492 - precision: 0.7207 - recall: 0.5945 - val_loss: 0.4475 - val_binary_accuracy: 0.7708 - val_auc: 0.8675 - val_precision: 0.7925 - val_recall: 0.5600 - lr: 2.5000e-04 - 186ms/epoch - 19ms/step\n",
      "Epoch 24/50\n",
      "10/10 - 0s - loss: 0.4561 - binary_accuracy: 0.7766 - auc: 0.8409 - precision: 0.6977 - recall: 0.5687 - val_loss: 0.4562 - val_binary_accuracy: 0.7604 - val_auc: 0.8561 - val_precision: 0.7455 - val_recall: 0.5616 - lr: 2.5000e-04 - 187ms/epoch - 19ms/step\n",
      "Epoch 25/50\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "10/10 - 0s - loss: 0.4560 - binary_accuracy: 0.7750 - auc: 0.8475 - precision: 0.7074 - recall: 0.5991 - val_loss: 0.4687 - val_binary_accuracy: 0.7448 - val_auc: 0.8521 - val_precision: 0.7500 - val_recall: 0.5455 - lr: 2.5000e-04 - 298ms/epoch - 30ms/step\n",
      "Epoch 26/50\n",
      "10/10 - 0s - loss: 0.4685 - binary_accuracy: 0.7750 - auc: 0.8385 - precision: 0.6788 - recall: 0.6150 - val_loss: 0.4445 - val_binary_accuracy: 0.7708 - val_auc: 0.8623 - val_precision: 0.7500 - val_recall: 0.5833 - lr: 1.2500e-04 - 208ms/epoch - 21ms/step\n",
      "Epoch 27/50\n",
      "10/10 - 0s - loss: 0.4460 - binary_accuracy: 0.7812 - auc: 0.8500 - precision: 0.7267 - recall: 0.5734 - val_loss: 0.4436 - val_binary_accuracy: 0.7760 - val_auc: 0.8618 - val_precision: 0.7547 - val_recall: 0.5714 - lr: 1.2500e-04 - 177ms/epoch - 18ms/step\n",
      "Epoch 28/50\n",
      "10/10 - 0s - loss: 0.4726 - binary_accuracy: 0.7631 - auc: 0.8307 - precision: 0.6761 - recall: 0.5749 - val_loss: 0.4415 - val_binary_accuracy: 0.7708 - val_auc: 0.8726 - val_precision: 0.7963 - val_recall: 0.5658 - lr: 1.2500e-04 - 308ms/epoch - 31ms/step\n",
      "Epoch 29/50\n",
      "10/10 - 0s - loss: 0.4242 - binary_accuracy: 0.7984 - auc: 0.8711 - precision: 0.7254 - recall: 0.6481 - val_loss: 0.4368 - val_binary_accuracy: 0.7812 - val_auc: 0.8690 - val_precision: 0.7843 - val_recall: 0.5634 - lr: 1.2500e-04 - 206ms/epoch - 21ms/step\n",
      "Epoch 30/50\n",
      "10/10 - 0s - loss: 0.4510 - binary_accuracy: 0.7719 - auc: 0.8479 - precision: 0.7027 - recall: 0.5882 - val_loss: 0.4417 - val_binary_accuracy: 0.7812 - val_auc: 0.8684 - val_precision: 0.7925 - val_recall: 0.5753 - lr: 1.2500e-04 - 315ms/epoch - 32ms/step\n",
      "Epoch 31/50\n",
      "10/10 - 0s - loss: 0.4397 - binary_accuracy: 0.7906 - auc: 0.8560 - precision: 0.7297 - recall: 0.6164 - val_loss: 0.4423 - val_binary_accuracy: 0.7812 - val_auc: 0.8689 - val_precision: 0.7925 - val_recall: 0.5753 - lr: 1.2500e-04 - 214ms/epoch - 21ms/step\n",
      "Epoch 32/50\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "10/10 - 0s - loss: 0.4424 - binary_accuracy: 0.7875 - auc: 0.8486 - precision: 0.6839 - recall: 0.5950 - val_loss: 0.4479 - val_binary_accuracy: 0.7760 - val_auc: 0.8696 - val_precision: 0.8000 - val_recall: 0.5789 - lr: 1.2500e-04 - 191ms/epoch - 19ms/step\n",
      "Epoch 33/50\n",
      "10/10 - 0s - loss: 0.4712 - binary_accuracy: 0.7672 - auc: 0.8401 - precision: 0.7181 - recall: 0.5844 - val_loss: 0.4507 - val_binary_accuracy: 0.7760 - val_auc: 0.8610 - val_precision: 0.7736 - val_recall: 0.5694 - lr: 6.2500e-05 - 348ms/epoch - 35ms/step\n",
      "Epoch 34/50\n",
      "10/10 - 0s - loss: 0.4362 - binary_accuracy: 0.7937 - auc: 0.8556 - precision: 0.7158 - recall: 0.6209 - val_loss: 0.4438 - val_binary_accuracy: 0.7760 - val_auc: 0.8634 - val_precision: 0.7736 - val_recall: 0.5694 - lr: 6.2500e-05 - 192ms/epoch - 19ms/step\n",
      "Epoch 35/50\n",
      "10/10 - 0s - loss: 0.4464 - binary_accuracy: 0.7781 - auc: 0.8518 - precision: 0.7097 - recall: 0.6000 - val_loss: 0.4459 - val_binary_accuracy: 0.7812 - val_auc: 0.8646 - val_precision: 0.7885 - val_recall: 0.5694 - lr: 6.2500e-05 - 174ms/epoch - 17ms/step\n",
      "Epoch 36/50\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "10/10 - 0s - loss: 0.4498 - binary_accuracy: 0.7844 - auc: 0.8465 - precision: 0.7102 - recall: 0.5896 - val_loss: 0.4391 - val_binary_accuracy: 0.7760 - val_auc: 0.8702 - val_precision: 0.7843 - val_recall: 0.5556 - lr: 6.2500e-05 - 199ms/epoch - 20ms/step\n",
      "Epoch 37/50\n",
      "10/10 - 0s - loss: 0.4495 - binary_accuracy: 0.7859 - auc: 0.8537 - precision: 0.7356 - recall: 0.6009 - val_loss: 0.4454 - val_binary_accuracy: 0.7760 - val_auc: 0.8654 - val_precision: 0.7818 - val_recall: 0.5811 - lr: 3.1250e-05 - 199ms/epoch - 20ms/step\n",
      "Epoch 38/50\n",
      "Restoring model weights from the end of the best epoch: 28.\n",
      "10/10 - 0s - loss: 0.4478 - binary_accuracy: 0.7750 - auc: 0.8493 - precision: 0.6964 - recall: 0.5571 - val_loss: 0.4424 - val_binary_accuracy: 0.7760 - val_auc: 0.8665 - val_precision: 0.7885 - val_recall: 0.5616 - lr: 3.1250e-05 - 182ms/epoch - 18ms/step\n",
      "Epoch 38: early stopping\n"
     ]
    }
   ],
   "source": [
    "# konfigurasi pipeline TFX\n",
    "components = [\n",
    "    example_gen,\n",
    "    statistics_gen,\n",
    "    schema_gen,\n",
    "    example_validator,\n",
    "    transform,\n",
    "    tuner,\n",
    "    trainer,\n",
    "    model_resolver,\n",
    "    evaluator,\n",
    "    pusher,\n",
    "]\n",
    "\n",
    "# Argumen untuk Apache Beam (direct runner)\n",
    "beam_pipeline_args = [\n",
    "    \"--direct_running_mode=in_memory\",\n",
    "    \"--direct_num_workers=1\",\n",
    "]\n",
    "\n",
    "# Buat objek Pipeline TFX\n",
    "pipeline = tfx_pipeline.Pipeline(\n",
    "    pipeline_name=PIPELINE_NAME,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    components=components,\n",
    "    enable_cache=True,\n",
    "    metadata_connection_config=metadata.sqlite_metadata_connection_config(\n",
    "        METADATA_PATH\n",
    "    ),\n",
    "    beam_pipeline_args=beam_pipeline_args,\n",
    ")\n",
    "\n",
    "# Jalankan pipeline dengan Apache Beam\n",
    "BeamDagRunner().run(pipeline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ab6a61",
   "metadata": {},
   "source": [
    "Pada tahap terakhir ini, saya menyusun dan mengeksekusi keseluruhan pipeline TFX. Saya terlebih dahulu menggabungkan seluruh komponen mulai dari ingestion, validasi, transformasi, tuning, training, evaluasi, hingga deployment ke dalam sebuah daftar components. Konfigurasi ini memastikan bahwa setiap komponen akan dijalankan secara berurutan dan saling terhubung melalui artefak yang mereka hasilkan.\n",
    "\n",
    "Selanjutnya, saya menetapkan argumen Apache Beam menggunakan direct runner, yang memungkinkan pipeline dijalankan secara lokal tanpa kluster terdistribusi. Dengan konfigurasi tersebut, saya kemudian membangun objek Pipeline yang mencakup nama pipeline, lokasi penyimpanan artefak, pengaturan metadata, serta daftar komponen yang akan dieksekusi. Fitur enable_cache=True saya aktifkan agar komponen tidak perlu dijalankan ulang apabila artefak yang sama sudah tersedia, sehingga proses eksekusi menjadi lebih efisien.\n",
    "\n",
    "Setelah seluruh konfigurasi siap, pipeline dijalankan menggunakan BeamDagRunner. Langkah ini mengeksekusi setiap komponen secara otomatis menggunakan Apache Beam, sehingga keseluruhan alur mulai dari data mentah hingga model siap deployment dapat berlangsung secara terstruktur, reproducible, dan sepenuhnya end-to-end."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final_mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
